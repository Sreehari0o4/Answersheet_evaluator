{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0bb2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. INSTALLATION: Tesseract and Python libraries\n",
    "\n",
    "# Install system packages for Tesseract OCR\n",
    "!apt-get update -qq\n",
    "!apt-get install -y tesseract-ocr libtesseract-dev\n",
    "\n",
    "# Install Python libraries\n",
    "!pip install -q pytesseract opencv-python pillow matplotlib\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from google.colab.patches import cv2_imshow\n",
    "from google.colab import files\n",
    "\n",
    "# Point pytesseract to the Tesseract executable installed above\n",
    "pytesseract.pytesseract.tesseract_cmd = \"/usr/bin/tesseract\"\n",
    "\n",
    "print(\"Tesseract version:\", pytesseract.get_tesseract_version())\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cefb66",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install Tesseract OCR and the required Python libraries, then import everything needed for OCR and image processing in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e956b96b",
   "metadata": {},
   "source": [
    "## 2. Image Upload\n",
    "\n",
    "Upload one or more scanned answer sheet images from your local machine using `google.colab.files.upload()`. The first uploaded image will be used in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfaecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. IMAGE UPLOAD: upload scanned answer sheet images\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Take the first uploaded file as the active image\n",
    "image_names = list(uploaded.keys())\n",
    "if not image_names:\n",
    "    raise RuntimeError(\"No image uploaded. Please run this cell again and upload an image.\")\n",
    "\n",
    "image_path = image_names[0]\n",
    "print(\"Using image:\", image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72147d9",
   "metadata": {},
   "source": [
    "## 3. Display Original Image\n",
    "\n",
    "Read the uploaded answer sheet image with OpenCV and display it using `cv2_imshow()` to verify that the correct file was loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b9e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DISPLAY ORIGINAL IMAGE\n",
    "image_bgr = cv2.imread(image_path)\n",
    "\n",
    "if image_bgr is None:\n",
    "    raise RuntimeError(f\"Failed to read image at path: {image_path}\")\n",
    "\n",
    "# OpenCV loads images in BGR color space; cv2_imshow can display it directly\n",
    "cv2_imshow(image_bgr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b605db",
   "metadata": {},
   "source": [
    "## 4. Image Preprocessing\n",
    "\n",
    "Convert the image to grayscale, reduce noise with Gaussian blur, and apply adaptive thresholding to create a high-contrast, binarized image suitable for OCR. The processed image will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc075769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. IMAGE PREPROCESSING: grayscale, blur, adaptive threshold\n",
    "# Reload the original image to ensure we start from a clean copy\n",
    "image_bgr = cv2.imread(image_path)\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply Gaussian blur for noise reduction\n",
    "blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "# Adaptive thresholding for binarization\n",
    "thresh = cv2.adaptiveThreshold(\n",
    "    blur,\n",
    "    255,\n",
    "    cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "    cv2.THRESH_BINARY,\n",
    "    11,\n",
    "    2\n",
    ")\n",
    "\n",
    "cv2_imshow(thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ae687",
   "metadata": {},
   "source": [
    "## 5. OCR Extraction\n",
    "\n",
    "Run Tesseract OCR on the preprocessed (thresholded) image using `pytesseract.image_to_string` with configuration `--oem 3 --psm 6`. The extracted text is printed clearly for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d591780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. OCR EXTRACTION: use pytesseract on preprocessed image\n",
    "custom_config = r\"--oem 3 --psm 6\"\n",
    "\n",
    "# pytesseract can work directly on the single-channel thresholded image\n",
    "extracted_text = pytesseract.image_to_string(thresh, config=custom_config)\n",
    "\n",
    "print(\"========== EXTRACTED TEXT ==========:\\n\")\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ed2f5",
   "metadata": {},
   "source": [
    "## 6. Save and Download Extracted Text\n",
    "\n",
    "Save the OCR output to a `.txt` file and provide a download link using `google.colab.files.download()` so you can easily export the extracted answer sheet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. SAVE OUTPUT: write extracted or corrected text to file and download\n",
    "output_filename = \"extracted_text.txt\"\n",
    "\n",
    "# Prefer corrected_text if it exists and is non-empty; otherwise fall back to extracted_text.\n",
    "if 'corrected_text' in globals() and isinstance(corrected_text, str) and corrected_text.strip():\n",
    "    text_to_save = corrected_text\n",
    "elif 'extracted_text' in globals() and isinstance(extracted_text, str) and extracted_text.strip():\n",
    "    text_to_save = extracted_text\n",
    "else:\n",
    "    raise RuntimeError(\"No OCR text available to save. Run the OCR (and optional correction) cells first.\")\n",
    "\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text_to_save)\n",
    "\n",
    "print(f\"Saved text to {output_filename}\")\n",
    "\n",
    "files.download(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc077bca",
   "metadata": {},
   "source": [
    "## 7. BERT-based Text Correction\n",
    "\n",
    "Use a transformer-based language model to analyse the OCR output and generate a corrected version of the extracted text. Run this after the OCR extraction cell and before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6742a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. BERT-STYLE TEXT CORRECTION USING A TRANSFORMER MODEL\n",
    "# This cell refines `extracted_text` and produces `corrected_text`.\n",
    "# Run the OCR extraction cell first so `extracted_text` is defined.\n",
    "\n",
    "!pip install -q transformers torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "model_name = \"prithivida/grammar_error_correcter_v1\"  # transformer-based text correction model\n",
    "corrector = pipeline(\"text2text-generation\", model=model_name, tokenizer=model_name)\n",
    "\n",
    "if 'extracted_text' not in globals():\n",
    "    raise RuntimeError(\"`extracted_text` is not defined. Run the OCR extraction cell first.\")\n",
    "\n",
    "input_text = extracted_text.strip()\n",
    "if not input_text:\n",
    "    raise ValueError(\"`extracted_text` is empty. Make sure OCR ran correctly.\")\n",
    "\n",
    "outputs = corrector(input_text, max_length=max(64, len(input_text.split()) * 3))\n",
    "corrected_text = outputs[0].get(\"generated_text\", outputs[0].get(\"translation_text\", input_text))\n",
    "\n",
    "print(\"=== ORIGINAL OCR TEXT ===\\n\")\n",
    "print(input_text)\n",
    "print(\"\\n=== CORRECTED TEXT ===\\n\")\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bcf868",
   "metadata": {},
   "source": [
    "# Deep Learning Handwriting OCR (TrOCR-based)\n",
    "\n",
    "The following cells implement a handwriting-optimized OCR pipeline using Microsoft's TrOCR model, which usually gives much higher accuracy on handwritten answer sheets than classic Tesseract or EasyOCR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b4815a",
   "metadata": {},
   "source": [
    "## 8. Install TrOCR Dependencies\n",
    "\n",
    "Install the required deep learning libraries and Hugging Face Transformers. Run this in Colab (GPU is recommended but not required for small images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0cb161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. INSTALL REQUIRED LIBRARIES FOR TrOCR\n",
    "!pip install -q transformers pillow torch torchvision torchaudio\n",
    "\n",
    "print(\"Installed Transformers + PyTorch dependencies for TrOCR.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b45de7c",
   "metadata": {},
   "source": [
    "## 9. Upload Answer Sheet Image (for TrOCR)\n",
    "\n",
    "Upload the handwritten answer sheet you want to process with the TrOCR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc66529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. UPLOAD ANSWER SHEET IMAGE (TrOCR)\n",
    "from google.colab import files\n",
    "\n",
    "uploaded_trocr = files.upload()\n",
    "\n",
    "if not uploaded_trocr:\n",
    "    raise RuntimeError(\"No image uploaded. Please upload a handwritten answer sheet.\")\n",
    "\n",
    "trocr_img_path = list(uploaded_trocr.keys())[0]\n",
    "print(\"Uploaded for TrOCR:\", trocr_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b3f4c0",
   "metadata": {},
   "source": [
    "## 10. Load and Preprocess Image for TrOCR\n",
    "\n",
    "Convert the uploaded image to grayscale, enhance contrast, and display it before feeding into the TrOCR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1833edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. LOAD IMAGE & PREPROCESS FOR TrOCR\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load image for TrOCR\n",
    "trocr_image = Image.open(trocr_img_path)\n",
    "\n",
    "# Convert to grayscale and auto-contrast to help handwriting OCR\n",
    "trocr_gray = ImageOps.grayscale(trocr_image)\n",
    "trocr_enhanced = ImageOps.autocontrast(trocr_gray)\n",
    "\n",
    "plt.imshow(trocr_enhanced, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Preprocessed image for TrOCR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69694ea3",
   "metadata": {},
   "source": [
    "## 11. Load TrOCR Handwriting Model\n",
    "\n",
    "Use Microsoft's `microsoft/trocr-small-handwritten` model, which is trained specifically for handwritten text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca81cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. LOAD TrOCR HANDWRITING MODEL\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "trocr_model_name = \"microsoft/trocr-small-handwritten\"\n",
    "processor = TrOCRProcessor.from_pretrained(trocr_model_name)\n",
    "trocr_model = VisionEncoderDecoderModel.from_pretrained(trocr_model_name)\n",
    "\n",
    "print(\"Loaded TrOCR model:\", trocr_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e908a88e",
   "metadata": {},
   "source": [
    "## 12. Perform TrOCR-based OCR Extraction\n",
    "\n",
    "Run the TrOCR model on the preprocessed image to get high-quality handwriting text extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf58979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. PERFORM TrOCR-BASED OCR EXTRACTION\n",
    "import torch\n",
    "\n",
    "# Upscale the preprocessed image to help the model read small handwriting\n",
    "scale_factor = 2\n",
    "w, h = trocr_enhanced.size\n",
    "trocr_up = trocr_enhanced.resize((w * scale_factor, h * scale_factor))\n",
    "\n",
    "# Ensure image is in RGB format for the processor\n",
    "trocr_rgb = trocr_up.convert(\"RGB\")\n",
    "\n",
    "# Prepare image for model\n",
    "trocr_pixel_values = processor(images=trocr_rgb, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "# Generate text with a slightly stronger decoder\n",
    "with torch.no_grad():\n",
    "    generated_ids = trocr_model.generate(\n",
    "        trocr_pixel_values,\n",
    "        num_beams=5,\n",
    "        max_length=256,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "trocr_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"===== TrOCR EXTRACTED TEXT =====\\n\")\n",
    "print(trocr_text)\n",
    "print(\"\\n===============================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1af4ea",
   "metadata": {},
   "source": [
    "## 13. Save TrOCR Output to File\n",
    "\n",
    "Save the TrOCR extracted text to a .txt file and download it from Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe0f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. SAVE TrOCR OUTPUT TO FILE\n",
    "from google.colab import files as colab_files\n",
    "\n",
    "trocr_output_filename = \"deep_ocr_output.txt\"\n",
    "\n",
    "with open(trocr_output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(trocr_text)\n",
    "\n",
    "print(f\"Saved TrOCR text to {trocr_output_filename}\")\n",
    "\n",
    "colab_files.download(trocr_output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2097ace5",
   "metadata": {},
   "source": [
    "## 14. Optional: Language-model Refinement of TrOCR Text\n",
    "\n",
    "Optionally refine `trocr_text` using a text-correction model (similar to BERT-style correction) to fix spelling and grammar issues in the raw OCR output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa05b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. OPTIONAL: REFINE TrOCR TEXT WITH A TEXT-CORRECTION MODEL\n",
    "!pip install -q transformers torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "if 'trocr_text' not in globals():\n",
    "    raise RuntimeError(\"`trocr_text` is not defined. Run the TrOCR extraction cell first.\")\n",
    "\n",
    "lm_model_name = \"prithivida/grammar_error_correcter_v1\"\n",
    "trocr_corrector = pipeline(\"text2text-generation\", model=lm_model_name, tokenizer=lm_model_name)\n",
    "\n",
    "raw_trocr = trocr_text.strip()\n",
    "if not raw_trocr:\n",
    "    raise ValueError(\"`trocr_text` is empty. Make sure TrOCR ran correctly.\")\n",
    "\n",
    "lm_outputs = trocr_corrector(raw_trocr, max_length=max(64, len(raw_trocr.split()) * 3))\n",
    "trocr_text_corrected = lm_outputs[0].get(\"generated_text\", lm_outputs[0].get(\"translation_text\", raw_trocr))\n",
    "\n",
    "print(\"=== RAW TrOCR TEXT ===\\n\")\n",
    "print(raw_trocr)\n",
    "print(\"\\n=== CORRECTED TrOCR TEXT ===\\n\")\n",
    "print(trocr_text_corrected)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
